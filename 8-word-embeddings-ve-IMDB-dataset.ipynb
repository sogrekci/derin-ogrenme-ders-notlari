{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Datset\n",
    "\n",
    "Şimdi kelimeler içeren dataset kullanarak bir örnek yapacağız, bunun için ***IMDB dataset*** adında bir veri setini kullanacağız. Bu veri setinde iki dosya mevcut; `reviews.txt` dosyasında IMDB sitesinde filmler için yapılan bazı yorumlar yer alıyor, `labels.txt` dosyasında da her bir yorum için `NEGATIVE` ya da `POSITIVE` kelimelerinden oluşan bir değer yer alıyor ve bu değer ilgili yorumun olmsuz veya olumlu bir yorum olması durumunu tarif ediyor. Aşağıda bu yorumları indirip görüntülüyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews.txt, labels.txt --> https://github.com/udacity/deep-learning/tree/master/sentiment-network\n",
    "\n",
    "def print_review(i):\n",
    "   print(\"[\" + labels[i] + \"]\\n\\n\" + reviews[i][:] + \"...\")\n",
    "\n",
    "g = open('data/reviews.txt','r') # What we know!\n",
    "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('data/labels.txt','r') # What we WANT to know!\n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEGATIVE]\n",
      "\n",
      "sorry everyone    i know this is supposed to be an  art  film   but wow  they should have handed out guns at the screening so people could blow their brains out and not watch . although the scene design and photographic direction was excellent  this story is too painful to watch . the absence of a sound track was brutal . the loooonnnnng shots were too long . how long can you watch two people just sitting there and talking  especially when the dialogue is two people complaining . i really had a hard time just getting through this film . the performances were excellent  but how much of that dark  sombre  uninspired  stuff can you take  the only thing i liked was maureen stapleton and her red dress and dancing scene . otherwise this was a ripoff of bergman . and i  m no fan f his either . i think anyone who says they enjoyed     hours of this is   well  lying .  ...\n"
     ]
    }
   ],
   "source": [
    "print_review(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi bu veri setini bir sinir ağına input olarak gönderilebilecek şekilde vektörize etmeliyiz, bunu yapmanın en popüler yollarından birisi ***one-hot encoding*** denlien yöntemdir. Bu yöntemde her bir yorumu `0` ve `1`'lerden oluşan bir vektör olarak saklamaktır; her bir vektörün boyutu toplam kelimelerin sayısı kadar olmalıdır ve yorumda içerilen kelimelerin yerinde `1`, yer almayan kelimelerin yerinde `0` yer almalıdır. Örneğin sözlüğümüz (toplam kelime havuzu) `cat`, `the`, `dog` ve `sat` kelimelerinden oluşsun (IMDB dışında basit bir örnek); bu kelimelerin her biri 4 elemanlı birer vektör olarak \n",
    "\n",
    "```\n",
    "[1, 0, 0, 0]   (cat)\n",
    "[0, 1, 0, 0]   (the)\n",
    "[0, 0, 1, 0]   (dog)\n",
    "[0, 0, 0, 1]   (sat)\n",
    "```\n",
    "\n",
    "olarak kodlanır. Bu durumda mesela `the cat sat` cümlesi one-hot olarak enkode edilirken her bir kelimenin vektörü toplanır ve `[1, 1, 0, 1]` vektörü elde edilir; bu kodlamada cümlede içerilen kelimelerin yerinde `1`, içerilmeyen kelimelerin yerinde `0` vardır. Aşağıdaki program bu işlemi özetliyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding for the sentence 'the cat sat':\n",
      "[1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "onehots = {}\n",
    "onehots['cat'] = np.array([1,0,0,0])\n",
    "onehots['the'] = np.array([0,1,0,0])\n",
    "onehots['dog'] = np.array([0,0,1,0])\n",
    "onehots['sat'] = np.array([0,0,0,1])\n",
    "\n",
    "sentence = ['the','cat','sat']\n",
    "encode = np.zeros(4, dtype=\"int\")\n",
    "\n",
    "for word in sentence:\n",
    "    encode += onehots[word]\n",
    "    \n",
    "print(\"One-hot encoding for the sentence '\" + \" \".join(sentence) + \"':\\n\" + str(encode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir kelime birden fazla kez yorumda yer alıyorsa onun yerinde `2`, `3` gibi sayılar yer alır, mesela `the cat the dog` cümlesi `[1, 2, 1, 0]` olarak kodlanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding for the sentence 'the cat the dog':\n",
      "[1 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "onehots = {}\n",
    "onehots['cat'] = np.array([1,0,0,0])\n",
    "onehots['the'] = np.array([0,1,0,0])\n",
    "onehots['dog'] = np.array([0,0,1,0])\n",
    "onehots['sat'] = np.array([0,0,0,1])\n",
    "\n",
    "sentence = ['the','cat','the', 'dog']\n",
    "encode = np.zeros(4, dtype=\"int\")\n",
    "\n",
    "for word in sentence:\n",
    "    encode += onehots[word]\n",
    "    \n",
    "print(\"One-hot encoding for the sentence '\" + \" \".join(sentence) + \"':\\n\" + str(encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "f = open('data/reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('data/labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list(map(lambda x:set(x.split(\" \")), raw_reviews))\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        if(len(word)>0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(list(set(sent_indices)))\n",
    "\n",
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == 'positive\\n':\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " '\\n',\n",
       " '.',\n",
       " 'a',\n",
       " 'absence',\n",
       " 'although',\n",
       " 'an',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'art',\n",
       " 'at',\n",
       " 'be',\n",
       " 'bergman',\n",
       " 'blow',\n",
       " 'brains',\n",
       " 'brutal',\n",
       " 'but',\n",
       " 'can',\n",
       " 'complaining',\n",
       " 'could',\n",
       " 'dancing',\n",
       " 'dark',\n",
       " 'design',\n",
       " 'dialogue',\n",
       " 'direction',\n",
       " 'dress',\n",
       " 'either',\n",
       " 'enjoyed',\n",
       " 'especially',\n",
       " 'everyone',\n",
       " 'excellent',\n",
       " 'f',\n",
       " 'fan',\n",
       " 'film',\n",
       " 'getting',\n",
       " 'guns',\n",
       " 'had',\n",
       " 'handed',\n",
       " 'hard',\n",
       " 'have',\n",
       " 'her',\n",
       " 'his',\n",
       " 'hours',\n",
       " 'how',\n",
       " 'i',\n",
       " 'is',\n",
       " 'just',\n",
       " 'know',\n",
       " 'liked',\n",
       " 'long',\n",
       " 'loooonnnnng',\n",
       " 'lying',\n",
       " 'm',\n",
       " 'maureen',\n",
       " 'much',\n",
       " 'no',\n",
       " 'not',\n",
       " 'of',\n",
       " 'only',\n",
       " 'otherwise',\n",
       " 'out',\n",
       " 'painful',\n",
       " 'people',\n",
       " 'performances',\n",
       " 'photographic',\n",
       " 'really',\n",
       " 'red',\n",
       " 'ripoff',\n",
       " 'says',\n",
       " 'scene',\n",
       " 'screening',\n",
       " 'shots',\n",
       " 'should',\n",
       " 'sitting',\n",
       " 'so',\n",
       " 'sombre',\n",
       " 'sorry',\n",
       " 'sound',\n",
       " 'stapleton',\n",
       " 'story',\n",
       " 'stuff',\n",
       " 'supposed',\n",
       " 'take',\n",
       " 'talking',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'there',\n",
       " 'they',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'this',\n",
       " 'through',\n",
       " 'time',\n",
       " 'to',\n",
       " 'too',\n",
       " 'track',\n",
       " 'two',\n",
       " 'uninspired',\n",
       " 'was',\n",
       " 'watch',\n",
       " 'well',\n",
       " 'were',\n",
       " 'when',\n",
       " 'who',\n",
       " 'wow',\n",
       " 'you'}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59721"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['uninspired']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[69124,\n",
       " 5129,\n",
       " 50193,\n",
       " 65557,\n",
       " 17429,\n",
       " 24600,\n",
       " 69661,\n",
       " 39967,\n",
       " 53793,\n",
       " 27690,\n",
       " 54831,\n",
       " 6194,\n",
       " 62003,\n",
       " 52276,\n",
       " 56378,\n",
       " 73795,\n",
       " 48200,\n",
       " 7246,\n",
       " 37459,\n",
       " 44120,\n",
       " 15962,\n",
       " 14943,\n",
       " 60518,\n",
       " 62067,\n",
       " 44662,\n",
       " 23158,\n",
       " 9848,\n",
       " 36479,\n",
       " 69769,\n",
       " 68752,\n",
       " 11422,\n",
       " 4785,\n",
       " 38066,\n",
       " 22715,\n",
       " 42171,\n",
       " 41149,\n",
       " 17605,\n",
       " 40646,\n",
       " 42185,\n",
       " 42708,\n",
       " 62166,\n",
       " 50395,\n",
       " 52962,\n",
       " 4324,\n",
       " 50404,\n",
       " 28412,\n",
       " 19198,\n",
       " 36608,\n",
       " 48903,\n",
       " 8969,\n",
       " 11530,\n",
       " 36620,\n",
       " 4381,\n",
       " 69418,\n",
       " 15152,\n",
       " 13108,\n",
       " 50485,\n",
       " 9528,\n",
       " 16191,\n",
       " 36163,\n",
       " 4932,\n",
       " 36675,\n",
       " 59721,\n",
       " 37197,\n",
       " 34638,\n",
       " 73551,\n",
       " 25425,\n",
       " 6998,\n",
       " 59743,\n",
       " 58720,\n",
       " 52078,\n",
       " 37749,\n",
       " 39797,\n",
       " 21883,\n",
       " 71548,\n",
       " 17291,\n",
       " 15245,\n",
       " 27023,\n",
       " 72080,\n",
       " 70039,\n",
       " 18843,\n",
       " 27039,\n",
       " 50593,\n",
       " 12194,\n",
       " 35745,\n",
       " 35234,\n",
       " 67493,\n",
       " 6567,\n",
       " 22955,\n",
       " 11197,\n",
       " 46533,\n",
       " 29642,\n",
       " 20941,\n",
       " 72655,\n",
       " 2518,\n",
       " 53721,\n",
       " 34783,\n",
       " 16866,\n",
       " 18916,\n",
       " 42469,\n",
       " 33771,\n",
       " 10225,\n",
       " 58355,\n",
       " 68087,\n",
       " 27128,\n",
       " 52734]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dataset[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uninspired'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[59721]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 Progress:95.99% Training Accuracy:0.8324583333333333%%\n",
      "Iter:1 Progress:95.99% Training Accuracy:0.8667916666666666%\n",
      "Test Accuracy:0.852\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "alpha, iterations = (0.01, 2)\n",
    "hidden_size = 100\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((len(vocab),hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,1)) - 0.1\n",
    "\n",
    "correct,total = (0,0)\n",
    "\n",
    "for iter in range(iterations): # train on first 24,000\n",
    "    for i in range(len(input_dataset)-1000):\n",
    "        x,y = (input_dataset[i],target_dataset[i])\n",
    "        layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0)) #embed + sigmoid\n",
    "        layer_2 = sigmoid(np.dot(layer_1,weights_1_2)) # linear + softmax\n",
    "        layer_2_delta = layer_2 - y # compare pred with truth\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) #backprop\n",
    "        weights_0_1[x] -= layer_1_delta * alpha\n",
    "        weights_1_2 -= np.outer(layer_1,layer_2_delta) * alpha\n",
    "        \n",
    "        if(np.abs(layer_2_delta) < 0.5):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        if(i % 10 == 9):\n",
    "            progress = str(i/float(len(input_dataset)))\n",
    "            sys.stdout.write('\\rIter:'+str(iter)\\\n",
    "                             +' Progress:'+progress[2:4]\\\n",
    "                             +'.'+progress[4:6]\\\n",
    "                             +'% Training Accuracy:'\\\n",
    "                             + str(correct/float(total)) + '%')\n",
    "    print()\n",
    "correct,total = (0,0)\n",
    "for i in range(len(input_dataset)-1000,len(input_dataset)):\n",
    "    x = input_dataset[i]\n",
    "    y = target_dataset[i]\n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0))\n",
    "    layer_2 = sigmoid(np.dot(layer_1,weights_1_2))\n",
    "    \n",
    "    if(np.abs(layer_2 - y) < 0.5):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(\"Test Accuracy:\" + str(correct / float(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
