{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Network ve Back Propagation\n",
    "\n",
    "\n",
    "Bu konuyu bir örnek üzerinde açıklayacağım. Farz edelim ki yayalar caddede karşıdan karşıya geçerken bir çeşit trafik lambasına göre hareket ediyor. Trafik lambası aşağıdaki gibi üzerinde üç lambadan oluşan bir düzenek olsun, bu lambalar aynı anda yanabiliyor. Hangileri yandığında durmalısınız, hangi kombinasyonlarda geçmelisiniz bunun kuralını siz biliyor olabilirsiniz. Fakat bilgisayara bu kuralı açıkça anlatmak yerine makine öğrenmesi yöntemleriyle durmaya veya geçmeye karar vermesini öğreteceksiniz.\n",
    "\n",
    "![](img/deep_learning_7.jpg)\n",
    "\n",
    "Bunun için biraz gözlem yaptınız. Bir süre boyunca lambaların çeşitli kombinasyonlarını ve bunlara karşın insanların ne yaptığını not alın. Mesela altı farklı kombinasyon için insanların durduğunu veya geçtiğini not aldınız ve aşağıdaki gibi bir tablo oluşturdunuz diyelim.\n",
    "\n",
    "![](img/deep_learning_8.jpg)\n",
    "\n",
    "Burada lambaların ve insanların durumlarını kodladık ayrıca; lambanın yanmasını `1`, yanmamasını `0`, insanların geçmesini `1` ve durmasını `0` ile kodladık. Mesela trafik lambası `0 1 1` (input) iken insanlar `1` (output) olmuş (yürümüş).\n",
    "\n",
    "Şimdi dataset (gözlem tablomuz) içindeki ilk veriyi kullanarak daha önce öğrendiğimiz yöntemle bir sinir ağı oluşturup  çalıştıralım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.03999999999999998 Prediction:-0.19999999999999996\n",
      "Error:0.03841599999999998 Prediction:-0.19599999999999995\n",
      "Error:0.03689472640000001 Prediction:-0.19208000000000003\n",
      "Error:0.03543369523456005 Prediction:-0.18823840000000014\n",
      "Error:0.03403052090327151 Prediction:-0.18447363200000022\n",
      "Error:0.03268291227550193 Prediction:-0.18078415936000014\n",
      "Error:0.031388668949392025 Prediction:-0.17716847617280007\n",
      "Error:0.030145677658996135 Prediction:-0.17362510664934416\n",
      "Error:0.028951908823699888 Prediction:-0.17015260451635728\n",
      "Error:0.02780541323428135 Prediction:-0.16674955242603007\n",
      "Error:0.02670431887020383 Prediction:-0.16341456137750954\n",
      "Error:0.02564682784294378 Prediction:-0.1601462701499594\n",
      "Error:0.024631213460363184 Prediction:-0.15694334474696015\n",
      "Error:0.023655817407332785 Prediction:-0.1538044778520209\n",
      "Error:0.022719047038002375 Prediction:-0.15072838829498036\n",
      "Error:0.02181937277529745 Prediction:-0.14771382052908066\n",
      "Error:0.02095532561339566 Prediction:-0.144759544118499\n",
      "Error:0.020125494719105186 Prediction:-0.141864353236129\n",
      "Error:0.019328525128228604 Prediction:-0.13902706617140637\n",
      "Error:0.01856311553315076 Prediction:-0.13624652484797828\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = np.array([0.5, 0.48, -0.7]) #random\n",
    "alpha = 0.01\n",
    "\n",
    "streetlights = np.array( [ [ 1, 0, 1 ],\n",
    "                           [ 0, 1, 1 ],\n",
    "                           [ 0, 0, 1 ],\n",
    "                           [ 1, 1, 1 ],\n",
    "                           [ 0, 1, 1 ],\n",
    "                           [ 1, 0, 1 ] ] )\n",
    "\n",
    "walk_stop = np.array( [0, 1, 0, 1, 1, 0] )\n",
    "\n",
    "input = streetlights[0] # [1,0,1]\n",
    "truth = walk_stop[0] # 0, yani \"stop\"\n",
    "\n",
    "for iteration in range(20):\n",
    "    prediction = input.dot(weights)\n",
    "    error = (truth - prediction)**2\n",
    "    delta = prediction - truth\n",
    "    weights = weights - (alpha*(input*delta)) #gradient descent\n",
    "    print(\"Error:\" + str(error) + \" Prediction:\" + str(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi bir de benzer bir sinir ağıyla dataset içinden tek bir veri yerine tüm dataset üzerinden öğrenme yapalım. Bunu yapmak basittir, sırasıyla tüm input değerleri için ayrı ayrı sinir ağı tahminde bulunur ve hataya göre her adımda weight değerlerini günceller. Bu şekilde her adımda ayrı bir input ile öğrenerek tüm dataseti öğrenme yöntemine ***stochastic gradient descent*** yöntemi denir. Daha önce gradient descent yöntemi ile veri setinin tek bir seferde öğrenildiğini biliyoruz, bu yöntemle weight değerleri tek seferde güncellenir; fakat stochastic gradient descent ile her adımda weight değerleri güncellenir. Daha sonra değineceğiz ama bu ikisinin (tek seferde tüm veri setini öğrenme ile inputları teker teker öğrenme, yani gradient descent ve stochastic gradient descent) arasında başka bir yol daha vardır. Bu yöntemde kaç adımda bir weight değerlerinin güncelleneceği bir ***batch size*** değeri belirleriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [ 0.52  0.48 -0.68],\t Prediction:-0.19999999999999996\n",
      "Weights: [ 0.52  0.6  -0.56],\t Prediction:-0.19999999999999996\n",
      "Weights: [ 0.52   0.6   -0.504],\t Prediction:-0.5599999999999999\n",
      "Weights: [ 0.5584  0.6384 -0.4656],\t Prediction:0.6160000000000001\n",
      "Weights: [ 0.5584   0.72112 -0.38288],\t Prediction:0.17279999999999995\n",
      "Weights: [ 0.540848  0.72112  -0.400432],\t Prediction:0.17552\n",
      "Error:2.6561231104\n",
      "\n",
      "Weights: [ 0.5268064  0.72112   -0.4144736],\t Prediction:0.14041599999999999\n",
      "Weights: [ 0.5268064   0.79045536 -0.34513824],\t Prediction:0.3066464\n",
      "Weights: [ 0.5268064   0.79045536 -0.31062442],\t Prediction:-0.34513824\n",
      "Weights: [ 0.52614267  0.78979163 -0.31128815],\t Prediction:1.006637344\n",
      "Weights: [ 0.52614267  0.84194128 -0.2591385 ],\t Prediction:0.4785034751999999\n",
      "Weights: [ 0.49944225  0.84194128 -0.28583891],\t Prediction:0.26700416768\n",
      "Error:0.9628701776715985\n",
      "\n",
      "Weights: [ 0.47808192  0.84194128 -0.30719925],\t Prediction:0.213603334144\n",
      "Weights: [ 0.47808192  0.88846708 -0.26067345],\t Prediction:0.5347420299776\n",
      "Weights: [ 0.47808192  0.88846708 -0.23460611],\t Prediction:-0.26067345110016\n",
      "Weights: [ 0.46488763  0.87527279 -0.24780039],\t Prediction:1.1319428845096962\n",
      "Weights: [ 0.46488763  0.91252555 -0.21054763],\t Prediction:0.6274723921901568\n",
      "Weights: [ 0.43945363  0.91252555 -0.23598163],\t Prediction:0.25433999330650114\n",
      "Error:0.5509165866836797\n",
      "\n",
      "Weights: [ 0.41910643  0.91252555 -0.25632883],\t Prediction:0.20347199464520088\n",
      "Weights: [ 0.41910643  0.94690588 -0.2219485 ],\t Prediction:0.6561967149569552\n",
      "Weights: [ 0.41910643  0.94690588 -0.19975365],\t Prediction:-0.221948503950995\n",
      "Weights: [ 0.40248056  0.93028001 -0.21637952],\t Prediction:1.166258650532124\n",
      "Weights: [ 0.40248056  0.95888996 -0.18776957],\t Prediction:0.7139004922542389\n",
      "Weights: [ 0.38100946  0.95888996 -0.20924067],\t Prediction:0.21471099528371604\n",
      "Error:0.36445836852222424\n",
      "\n",
      "Weights: [ 0.36383258  0.95888996 -0.22641755],\t Prediction:0.17176879622697283\n",
      "Weights: [ 0.36383258  0.98564272 -0.19966479],\t Prediction:0.7324724146523222\n",
      "Weights: [ 0.36383258  0.98564272 -0.17969831],\t Prediction:-0.19966478845083285\n",
      "Weights: [ 0.34685488  0.96866502 -0.19667601],\t Prediction:1.1697769945341199\n",
      "Weights: [ 0.34685488  0.99146612 -0.17387491],\t Prediction:0.7719890116601171\n",
      "Weights: [ 0.32955689  0.99146612 -0.19117291],\t Prediction:0.17297997428859369\n",
      "Error:0.2516768662079895\n",
      "\n",
      "Weights: [ 0.31571849  0.99146612 -0.20501131],\t Prediction:0.13838397943087496\n",
      "Weights: [ 0.31571849  1.01282064 -0.18365679],\t Prediction:0.7864548139561468\n",
      "Weights: [ 0.31571849  1.01282064 -0.16529111],\t Prediction:-0.1836567869927348\n",
      "Weights: [ 0.29939369  0.99649584 -0.18161591],\t Prediction:1.163248019006011\n",
      "Weights: [ 0.29939369  1.01500784 -0.1631039 ],\t Prediction:0.8148799260629888\n",
      "Weights: [ 0.28576471  1.01500784 -0.17673288],\t Prediction:0.1362897844408577\n",
      "Error:0.17797575048089034\n",
      "\n",
      "Weights: [ 0.27486153  1.01500784 -0.18763606],\t Prediction:0.10903182755268614\n",
      "Weights: [ 0.27486153  1.03227067 -0.17037324],\t Prediction:0.8273717796510367\n",
      "Weights: [ 0.27486153  1.03227067 -0.15333592],\t Prediction:-0.17037324196481937\n",
      "Weights: [ 0.2594819   1.01689104 -0.16871555],\t Prediction:1.1537962739591756\n",
      "Weights: [ 0.2594819   1.03207349 -0.15353309],\t Prediction:0.8481754931254761\n",
      "Weights: [ 0.24888702  1.03207349 -0.16412797],\t Prediction:0.1059488041691444\n",
      "Error:0.12864460733422164\n",
      "\n",
      "Weights: [ 0.24041111  1.03207349 -0.17260388],\t Prediction:0.0847590433353155\n",
      "Weights: [ 0.24041111  1.04612653 -0.15855084],\t Prediction:0.859469609749935\n",
      "Weights: [ 0.24041111  1.04612653 -0.14269576],\t Prediction:-0.1585508402022421\n",
      "Weights: [ 0.22602693  1.03174234 -0.15707994],\t Prediction:1.1438418857156731\n",
      "Weights: [ 0.22602693  1.0442761  -0.14454618],\t Prediction:0.8746623946770374\n",
      "Weights: [ 0.21787885  1.0442761  -0.15269426],\t Prediction:0.08148074110264475\n",
      "Error:0.09511036950476208\n",
      "\n",
      "Weights: [ 0.21136039  1.0442761  -0.15921272],\t Prediction:0.06518459288211581\n",
      "Weights: [ 0.21136039  1.05576976 -0.14771906],\t Prediction:0.8850633823431538\n",
      "Weights: [ 0.21136039  1.05576976 -0.13294715],\t Prediction:-0.14771905585408038\n",
      "Weights: [ 0.19794209  1.04235146 -0.14636545],\t Prediction:1.1341830033853888\n",
      "Weights: [ 0.19794209  1.05275286 -0.13596405],\t Prediction:0.8959860107828534\n",
      "Weights: [ 0.19174429  1.05275286 -0.14216186],\t Prediction:0.0619780399014222\n",
      "Error:0.07194564247043436\n",
      "\n",
      "Weights: [ 0.18678604  1.05275286 -0.1471201 ],\t Prediction:0.04958243192113776\n",
      "Weights: [ 0.18678604  1.06218958 -0.13768338],\t Prediction:0.9056327614440267\n",
      "Weights: [ 0.18678604  1.06218958 -0.12391504],\t Prediction:-0.13768337501215525\n",
      "Weights: [ 0.17427999  1.04968353 -0.1364211 ],\t Prediction:1.1250605910610996\n",
      "Weights: [ 0.17427999  1.05835728 -0.12774734],\t Prediction:0.9132624284442169\n",
      "Weights: [ 0.16962672  1.05835728 -0.1324006 ],\t Prediction:0.04653264583708144\n",
      "Error:0.05564914990717743\n",
      "\n",
      "Weights: [ 0.16590411  1.05835728 -0.13612322],\t Prediction:0.03722611666966513\n",
      "Weights: [ 0.16590411  1.06613388 -0.12834662],\t Prediction:0.922234066504699\n",
      "Weights: [ 0.16590411  1.06613388 -0.11551196],\t Prediction:-0.12834662236261596\n",
      "Weights: [ 0.15425151  1.05448127 -0.12716456],\t Prediction:1.116526024487899\n",
      "Weights: [ 0.15425151  1.0617496  -0.11989623],\t Prediction:0.9273167105424409\n",
      "Weights: [ 0.15081598  1.0617496  -0.12333176],\t Prediction:0.03435527296969987\n",
      "Error:0.04394763937673939\n",
      "\n",
      "Weights: [ 0.14806756  1.0617496  -0.12608018],\t Prediction:0.027484218375759886\n",
      "Weights: [ 0.14806756  1.06818266 -0.11964712],\t Prediction:0.9356694192994068\n",
      "Weights: [ 0.14806756  1.06818266 -0.10768241],\t Prediction:-0.11964712469387503\n",
      "Weights: [ 0.13721078  1.05732588 -0.11853919],\t Prediction:1.1085678053734553\n",
      "Weights: [ 0.13721078  1.06344721 -0.11241786],\t Prediction:0.9387866868342218\n",
      "Weights: [ 0.13473149  1.06344721 -0.11489715],\t Prediction:0.024792915481941458\n",
      "Error:0.035357967050948465\n",
      "\n",
      "Weights: [ 0.13274805  1.06344721 -0.11688059],\t Prediction:0.019834332385553155\n",
      "Weights: [ 0.13274805  1.06879055 -0.11153725],\t Prediction:0.946566624680628\n",
      "Weights: [ 0.13274805  1.06879055 -0.10038352],\t Prediction:-0.11153724870006754\n",
      "Weights: [ 0.12263254  1.05867504 -0.11049903],\t Prediction:1.1011550767549563\n",
      "Weights: [ 0.12263254  1.06385744 -0.10531663],\t Prediction:0.948176009263518\n",
      "Weights: [ 0.12090095  1.06385744 -0.10704822],\t Prediction:0.017315912033043404\n",
      "Error:0.02890700056547436\n",
      "\n",
      "Weights: [ 0.11951568  1.06385744 -0.1084335 ],\t Prediction:0.013852729626434732\n",
      "Weights: [ 0.11951568  1.06831505 -0.10397589],\t Prediction:0.9554239432448665\n",
      "Weights: [ 0.11951568  1.06831505 -0.0935783 ],\t Prediction:-0.10397589092234266\n",
      "Weights: [ 0.11009044  1.0588898  -0.10300354],\t Prediction:1.0942524239871314\n",
      "Weights: [ 0.11009044  1.06330118 -0.09859217],\t Prediction:0.9558862588907013\n",
      "Weights: [ 0.10894061  1.06330118 -0.099742  ],\t Prediction:0.011498267782398985\n",
      "Error:0.023951660591138853\n",
      "\n",
      "Weights: [ 0.10802075  1.06330118 -0.10066186],\t Prediction:0.009198614225919194\n",
      "Weights: [ 0.10802075  1.06703725 -0.09692579],\t Prediction:0.9626393189117293\n",
      "Weights: [ 0.10802075  1.06703725 -0.08723321],\t Prediction:-0.09692579020989642\n",
      "Weights: [ 0.09923827  1.05825477 -0.09601569],\t Prediction:1.087824783849832\n",
      "Weights: [ 0.09923827  1.06203086 -0.0922396 ],\t Prediction:0.9622390773804066\n",
      "Weights: [ 0.0985384   1.06203086 -0.09293946],\t Prediction:0.006998674002545002\n",
      "Error:0.020063105176016144\n",
      "\n",
      "Weights: [ 0.09797851  1.06203086 -0.09349936],\t Prediction:0.005598939202035996\n",
      "Weights: [ 0.09797851  1.06517771 -0.09035251],\t Prediction:0.9685315005838672\n",
      "Weights: [ 0.09797851  1.06517771 -0.08131726],\t Prediction:-0.09035250869077546\n",
      "Weights: [ 0.08979461  1.05699381 -0.08950115],\t Prediction:1.0818389613301889\n",
      "Weights: [ 0.08979461  1.06024455 -0.08625042],\t Prediction:0.9674926590701334\n",
      "Weights: [ 0.08944019  1.06024455 -0.08660484],\t Prediction:0.003544193999268516\n",
      "Error:0.016952094519447087\n",
      "\n",
      "Weights: [ 0.08915666  1.06024455 -0.08688837],\t Prediction:0.0028353551994148157\n",
      "Weights: [ 0.08915666  1.06290893 -0.08422399],\t Prediction:0.9733561723362383\n",
      "Weights: [ 0.08915666  1.06290893 -0.07580159],\t Prediction:-0.0842239920152223\n",
      "Weights: [ 0.08153026  1.05528253 -0.08342799],\t Prediction:1.0762639960116431\n",
      "Weights: [ 0.08153026  1.05809708 -0.08061345],\t Prediction:0.9718545378681842\n",
      "Weights: [ 0.08143858  1.05809708 -0.08070513],\t Prediction:0.0009168131382832068\n",
      "Error:0.014420818295271236\n",
      "\n",
      "Weights: [ 0.08136523  1.05809708 -0.08077847],\t Prediction:0.0007334505106265654\n",
      "Weights: [ 0.08136523  1.06036522 -0.07851033],\t Prediction:0.9773186039296565\n",
      "Weights: [ 0.08136523  1.06036522 -0.0706593 ],\t Prediction:-0.07851033295953944\n",
      "Weights: [ 0.07425812  1.0532581  -0.07776641],\t Prediction:1.0710711494147542\n",
      "Weights: [ 0.07425812  1.05570893 -0.07531558],\t Prediction:0.9754916865567282\n",
      "Weights: [ 0.07436386  1.05570893 -0.07520984],\t Prediction:-0.0010574652271341245\n",
      "Error:0.012331739998443648\n",
      "\n",
      "Weights: [ 0.07444846  1.05570893 -0.07512524],\t Prediction:-0.0008459721817072885\n",
      "Weights: [ 0.07444846  1.05765056 -0.07318361],\t Prediction:0.9805836929862668\n",
      "Weights: [ 0.07444846  1.05765056 -0.06586525],\t Prediction:-0.07318360881847627\n",
      "Weights: [ 0.06782508  1.05102719 -0.07248863],\t Prediction:1.066233777045345\n",
      "Weights: [ 0.06782508  1.05317333 -0.07034248],\t Prediction:0.9785385598617921\n",
      "Weights: [ 0.06807682  1.05317333 -0.07009074],\t Prediction:-0.0025173975573930946\n",
      "Error:0.010587393171639842\n",
      "\n",
      "Weights: [ 0.06827822  1.05317333 -0.06988935],\t Prediction:-0.002013918045914484\n",
      "Weights: [ 0.06827822  1.05484493 -0.06821775],\t Prediction:0.9832839794497644\n",
      "Weights: [ 0.06827822  1.05484493 -0.06139597],\t Prediction:-0.06821774801198803\n",
      "Weights: [ 0.0621055   1.04867221 -0.06756869],\t Prediction:1.0617271739912904\n",
      "Weights: [ 0.0621055   1.05056186 -0.06567904],\t Prediction:0.9811035235627523\n",
      "Weights: [ 0.06246285  1.05056186 -0.06532169],\t Prediction:-0.0035735447350425317\n",
      "Error:0.009117233405426495\n",
      "\n",
      "Weights: [ 0.06274874  1.05056186 -0.0650358 ],\t Prediction:-0.002858835788034024\n",
      "Weights: [ 0.06274874  1.05200926 -0.06358841],\t Prediction:0.9855260569025094\n",
      "Weights: [ 0.06274874  1.05200926 -0.05722957],\t Prediction:-0.06358841060413677\n",
      "Weights: [ 0.05699589  1.04625641 -0.06298241],\t Prediction:1.05752842286588\n",
      "Weights: [ 0.05699589  1.04792901 -0.06130981],\t Prediction:0.9832740020092452\n",
      "Weights: [ 0.05742729  1.04792901 -0.06087842],\t Prediction:-0.004313918034364962\n",
      "Error:0.00786904226904208\n",
      "\n",
      "Weights: [ 0.0577724   1.04792901 -0.06053331],\t Prediction:-0.003451134427491974\n",
      "Weights: [ 0.0577724   1.04918944 -0.05927288],\t Prediction:0.9873957068535818\n",
      "Weights: [ 0.0577724   1.04918944 -0.05334559],\t Prediction:-0.059272877470408075\n",
      "Weights: [ 0.05241077  1.04382782 -0.05870721],\t Prediction:1.0536162524729626\n",
      "Weights: [ 0.05241077  1.04531576 -0.05721928],\t Prediction:0.9851206027353137\n",
      "Weights: [ 0.05289162  1.04531576 -0.05673843],\t Prediction:-0.004808501248434842\n",
      "Error:0.006803273214640502\n",
      "\n",
      "Weights: [ 0.0532763   1.04531576 -0.05635375],\t Prediction:-0.0038468009987478735\n",
      "Weights: [ 0.0532763   1.04641956 -0.05524995],\t Prediction:0.9889620124129692\n",
      "Weights: [ 0.0532763   1.04641956 -0.04972495],\t Prediction:-0.05524994626077355\n",
      "Weights: [ 0.04827921  1.04142247 -0.05472204],\t Prediction:1.049970908776931\n",
      "Weights: [ 0.04827921  1.04275242 -0.05339208],\t Prediction:0.9867004228010665\n",
      "Weights: [ 0.0487905   1.04275242 -0.0528808 ],\t Prediction:-0.005112871449710697\n",
      "Error:0.005889303541837786\n",
      "\n",
      "Weights: [ 0.04919953  1.04275242 -0.05247177],\t Prediction:-0.004090297159768559\n",
      "Weights: [ 0.04919953  1.04372436 -0.05149983],\t Prediction:0.9902806551018011\n",
      "Weights: [ 0.04919953  1.04372436 -0.04634985],\t Prediction:-0.051499833441728114\n",
      "Weights: [ 0.04454213  1.03906695 -0.05100725],\t Prediction:1.0465740376293469\n",
      "Weights: [ 0.04454213  1.04026098 -0.04981322],\t Prediction:0.9880596998997442\n",
      "Weights: [ 0.04506924  1.04026098 -0.04928611],\t Prediction:-0.0052710974096659285\n",
      "Error:0.0051029252561172675\n",
      "\n",
      "Weights: [ 0.04549092  1.04026098 -0.04886443],\t Prediction:-0.004216877927732746\n",
      "Weights: [ 0.04549092  1.04112133 -0.04800408],\t Prediction:0.9913965574535352\n",
      "Weights: [ 0.04549092  1.04112133 -0.04320367],\t Prediction:-0.048004082062078055\n",
      "Weights: [ 0.04115007  1.03678047 -0.04754453],\t Prediction:1.043408578143574\n",
      "Weights: [ 0.04115007  1.03785688 -0.04646813],\t Prediction:0.9892359385403211\n",
      "Weights: [ 0.04168187  1.03785688 -0.04593632],\t Prediction:-0.005318059364078823\n",
      "Error:0.004424644608684828\n",
      "\n",
      "Weights: [ 0.04210732  1.03785688 -0.04551087],\t Prediction:-0.0042544474912630525\n",
      "Weights: [ 0.04210732  1.03862228 -0.04474547],\t Prediction:0.992346001517791\n",
      "Weights: [ 0.04210732  1.03862228 -0.04027093],\t Prediction:-0.044745474990504665\n",
      "Weights: [ 0.03806145  1.03457641 -0.04431679],\t Prediction:1.0404586655589985\n",
      "Weights: [ 0.03806145  1.03555045 -0.04334276],\t Prediction:0.9902596156014837\n",
      "Weights: [ 0.03858958  1.03555045 -0.04281463],\t Prediction:-0.005281305317687134\n",
      "Error:0.0038385124412518303\n",
      "\n",
      "Weights: [ 0.03901209  1.03555045 -0.04239212],\t Prediction:-0.0042250442541497055\n",
      "Weights: [ 0.03901209  1.03623462 -0.04170795],\t Prediction:0.9931583274383705\n",
      "Weights: [ 0.03901209  1.03623462 -0.03753716],\t Prediction:-0.041707953394155776\n",
      "Weights: [ 0.03524113  1.03246366 -0.04130811],\t Prediction:1.0377095425371112\n",
      "Weights: [ 0.03524113  1.03334811 -0.04042367],\t Prediction:0.9911555487826897\n",
      "Weights: [ 0.03575938  1.03334811 -0.03990541],\t Prediction:-0.005182536193432452\n",
      "Error:0.0033313054558089675\n",
      "\n",
      "Weights: [ 0.03617399  1.03334811 -0.03949081],\t Prediction:-0.004146028954745959\n",
      "Weights: [ 0.03617399  1.03396238 -0.03887654],\t Prediction:0.9938572955409696\n",
      "Weights: [ 0.03617399  1.03396238 -0.03498889],\t Prediction:-0.03887654022599941\n",
      "Weights: [ 0.03265924  1.03044763 -0.03850363],\t Prediction:1.0351474779634813\n",
      "Weights: [ 0.03265924  1.03125323 -0.03769803],\t Prediction:0.9919439948626794\n",
      "Weights: [ 0.03316312  1.03125323 -0.03719415],\t Prediction:-0.00503879377425797\n",
      "Error:0.0028919416227737734\n",
      "\n",
      "Weights: [ 0.03356622  1.03125323 -0.03679105],\t Prediction:-0.004031035019406375\n",
      "Weights: [ 0.03356622  1.03180701 -0.03623727],\t Prediction:0.9944621787695098\n",
      "Weights: [ 0.03356622  1.03180701 -0.03261354],\t Prediction:-0.03623726848360008\n",
      "Weights: [ 0.03029025  1.02853104 -0.03588951],\t Prediction:1.032759692455092\n",
      "Weights: [ 0.03029025  1.02926689 -0.03515366],\t Prediction:0.9926415313729495\n",
      "Weights: [ 0.03077659  1.02926689 -0.03466732],\t Prediction:-0.004863410672429416\n",
      "Error:0.002511053608117256\n",
      "\n",
      "Weights: [ 0.03116567  1.02926689 -0.03427825],\t Prediction:-0.003890728537943533\n",
      "Weights: [ 0.03116567  1.02976803 -0.03377711],\t Prediction:0.9949886390193969\n",
      "Weights: [ 0.03116567  1.02976803 -0.0303994 ],\t Prediction:-0.03377711399894662\n",
      "Weights: [ 0.02811224  1.0267146  -0.03345283],\t Prediction:1.0305342898820642\n",
      "Weights: [ 0.02811224  1.02738842 -0.03277901],\t Prediction:0.9932617646389992\n",
      "Weights: [ 0.02857892  1.02738842 -0.03231233],\t Prediction:-0.004666769772712614\n",
      "Error:0.0021806703520253884\n",
      "\n",
      "Weights: [ 0.02895226  1.02738842 -0.03193899],\t Prediction:-0.003733415818170091\n",
      "Weights: [ 0.02895226  1.02784348 -0.03148393],\t Prediction:0.9954494302702878\n",
      "Weights: [ 0.02895226  1.02784348 -0.02833554],\t Prediction:-0.03148393251909879\n",
      "Weights: [ 0.02610624  1.02499746 -0.03118156],\t Prediction:1.0284601943056741\n",
      "Weights: [ 0.02610624  1.02561587 -0.03056315],\t Prediction:0.9938158986070053\n",
      "Weights: [ 0.02655193  1.02561587 -0.03011746],\t Prediction:-0.004456911151490314\n",
      "Error:0.0018939739123713475\n",
      "\n",
      "Weights: [ 0.02690848  1.02561587 -0.0297609 ],\t Prediction:-0.003565528921192253\n",
      "Weights: [ 0.02690848  1.02603037 -0.0293464 ],\t Prediction:0.9958549628928723\n",
      "Weights: [ 0.02690848  1.02603037 -0.02641176],\t Prediction:-0.029346400840475826\n",
      "Weights: [ 0.02425577  1.02337766 -0.02906447],\t Prediction:1.0265270918125804\n",
      "Weights: [ 0.02425577  1.02394634 -0.02849579],\t Prediction:0.9943131920358295\n",
      "Weights: [ 0.02467977  1.02394634 -0.02807179],\t Prediction:-0.004240016908292479\n",
      "Error:0.0016451096996342332\n",
      "\n",
      "Weights: [ 0.02501898  1.02394634 -0.02773259],\t Prediction:-0.0033920135266339822\n",
      "Weights: [ 0.02501898  1.02432497 -0.02735396],\t Prediction:0.9962137566721563\n",
      "Weights: [ 0.02501898  1.02432497 -0.02461857],\t Prediction:-0.02735396176499221\n",
      "Weights: [ 0.02254644  1.02185243 -0.0270911 ],\t Prediction:1.0247253767906936\n",
      "Weights: [ 0.02254644  1.0223763  -0.02656724],\t Prediction:0.9947613261560856\n",
      "Weights: [ 0.02294852  1.0223763  -0.02616516],\t Prediction:-0.004020798285770878\n",
      "Error:0.0014290353984827077\n",
      "\n",
      "Weights: [ 0.02327018  1.0223763  -0.02584349],\t Prediction:-0.003216638628616701\n",
      "Weights: [ 0.02327018  1.02272302 -0.02549677],\t Prediction:0.9965328046163073\n",
      "Weights: [ 0.02327018  1.02272302 -0.0229471 ],\t Prediction:-0.025496772653362886\n",
      "Weights: [ 0.02096557  1.02041841 -0.02525171],\t Prediction:1.0230461022472208\n",
      "Weights: [ 0.02096557  1.02090174 -0.02476838],\t Prediction:0.9951667005089379\n",
      "Weights: [ 0.02134585  1.02090174 -0.0243881 ],\t Prediction:-0.0038028045995257484\n",
      "Error:0.0012413985592149145\n",
      "\n",
      "Weights: [ 0.02165008  1.02090174 -0.02408387],\t Prediction:-0.003042243679620596\n",
      "Weights: [ 0.02165008  1.02121995 -0.02376566],\t Prediction:0.996817865235065\n",
      "Weights: [ 0.02165008  1.02121995 -0.02138909],\t Prediction:-0.023765657359234325\n",
      "Weights: [ 0.01950198  1.01907186 -0.02353719],\t Prediction:1.0214809338160067\n",
      "Weights: [ 0.01950198  1.01951839 -0.02309065],\t Prediction:0.995534671160774\n",
      "Weights: [ 0.01986085  1.01951839 -0.02273179],\t Prediction:-0.0035886696105582767\n",
      "Error:0.0010784359268087556\n",
      "\n",
      "Weights: [ 0.02014794  1.01951839 -0.02244469],\t Prediction:-0.0028709356884466207\n",
      "Weights: [ 0.02014794  1.01981102 -0.02215206],\t Prediction:0.9970736974585198\n",
      "Weights: [ 0.02014794  1.01981102 -0.01993686],\t Prediction:-0.022152061336940452\n",
      "Weights: [ 0.01814573  1.01780881 -0.02193907],\t Prediction:1.0200221071408409\n",
      "Weights: [ 0.01814573  1.01822183 -0.02152604],\t Prediction:0.9958697426723416\n",
      "Weights: [ 0.01848376  1.01822183 -0.02118801],\t Prediction:-0.0033803078583175654\n",
      "Error:0.0009368896209360312\n",
      "\n",
      "Weights: [ 0.01875419  1.01822183 -0.02091758],\t Prediction:-0.0027042462866540516\n",
      "Weights: [ 0.01875419  1.01849141 -0.02064801],\t Prediction:0.9973042495523706\n",
      "Weights: [ 0.01875419  1.01849141 -0.01858321],\t Prediction:-0.02064800972530455\n",
      "Weights: [ 0.01688795  1.01662517 -0.02044945],\t Prediction:1.018662388355171\n",
      "Weights: [ 0.01688795  1.0170076  -0.02006702],\t Prediction:0.9961757229433927\n",
      "Weights: [ 0.01720586  1.0170076  -0.01974911],\t Prediction:-0.0031790709774033414\n",
      "Error:0.0008139366504753339\n",
      "\n",
      "Weights: [ 0.01746018  1.0170076  -0.01949479],\t Prediction:-0.002543256781922673\n",
      "Weights: [ 0.01746018  1.01725632 -0.01924607],\t Prediction:0.9975128111306469\n",
      "Weights: [ 0.01746018  1.01725632 -0.01732146],\t Prediction:-0.019246068219762574\n",
      "Weights: [ 0.01572068  1.01551681 -0.01906097],\t Prediction:1.0173950374076535\n",
      "Weights: [ 0.01572068  1.01587123 -0.01870655],\t Prediction:0.9964558482449631\n",
      "Weights: [ 0.01601927  1.01587123 -0.01840796],\t Prediction:-0.0029858720226535913\n",
      "Error:0.0007071291752624441\n",
      "\n",
      "Weights: [ 0.01625813  1.01587123 -0.01816909],\t Prediction:-0.002388697618122871\n",
      "Weights: [ 0.01625813  1.01610102 -0.01793931],\t Prediction:0.9977021355600483\n",
      "Weights: [ 0.01625813  1.01610102 -0.01614538],\t Prediction:-0.01793930655497516\n",
      "Weights: [ 0.01463676  1.01447964 -0.01776675],\t Prediction:1.0162137740080082\n",
      "Weights: [ 0.01463676  1.01480835 -0.01743804],\t Prediction:0.9967128843019345\n",
      "Weights: [ 0.01491689  1.01480835 -0.01715791],\t Prediction:-0.0028012842268006904\n",
      "Error:0.0006143435674831474\n",
      "\n",
      "Weights: [ 0.01514099  1.01480835 -0.01693381],\t Prediction:-0.0022410273814405524\n",
      "Weights: [ 0.01514099  1.0150209  -0.01672126],\t Prediction:0.9978745386023716\n",
      "Weights: [ 0.01514099  1.0150209  -0.01504914],\t Prediction:-0.016721264429884947\n",
      "Weights: [ 0.01362971  1.01350962 -0.01656041],\t Prediction:1.0151127459893812\n",
      "Weights: [ 0.01362971  1.0138147  -0.01625533],\t Prediction:0.9969492081270097\n",
      "Weights: [ 0.01389228  1.0138147  -0.01599277],\t Prediction:-0.0026256193329783125\n",
      "Error:0.00053373677328488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = np.array([0.5,0.48,-0.7])\n",
    "alpha = 0.1\n",
    "\n",
    "streetlights = np.array( [[ 1, 0, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 0, 0, 1 ],\n",
    "                          [ 1, 1, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 1, 0, 1 ] ] )\n",
    "\n",
    "walk_stop = np.array( [0, 1, 0, 1, 1, 0] )\n",
    "\n",
    "input = streetlights[0] # [1,0,1]\n",
    "goal_prediction = walk_stop[0] # 0\n",
    "\n",
    "for iteration in range(40):\n",
    "    error_total = 0\n",
    "    for i in range(len(walk_stop)):\n",
    "        input = streetlights[i]\n",
    "        truth = walk_stop[i]\n",
    "        prediction = input.dot(weights)\n",
    "        error = (truth - prediction)**2\n",
    "        error_total += error\n",
    "        delta = prediction - truth\n",
    "        weights = weights - (alpha*(input*delta)) #stochastic gradient descent\n",
    "        print(\"Weights: \" + str(weights) + \",\\t Prediction:\" + str(prediction))\n",
    "    print(\"Error:\" + str(error_total) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukarıdaki programın çıktısını incelerseniz weight değerlerinden ilk ve üçüncü lambanın değeri sıfıra, ortadaki lambanın weight değeri ise bire yakınsıyor. Gerçekten gözlem tablonuzu incelerseniz bunu anlamlandırabilirsiniz; zira ortadaki lamba yanıyorsa yürüyebilirsiniz, yanmıyorsa beklersiniz. Sonucu sadece ortadaki lamba belirliyor!\n",
    "\n",
    "![](img/deep_learning_12.jpg)\n",
    "\n",
    "Dolayısıyla sinir ağımız `1,0,1` input değeri ile `0` (stop) outpur değeri arasında bir ilişki kurabilecek şekilde weight değerleri aradı. Dikkat ederseniz ortadaki lambanın weight değeri sonuçta 1 olarak, diğer weight değerleri 0 olarak belirlenmiş. Bunun anlamı şudur; şinir ağımız ortadaki input ile sonuç arasında güçlü bir ilişki fark etmiş; bu ilişkiye istatistiksel olarak **korelasyon** diyoruz. Yüksek korelasyona sahip çiftler arasında yüksek weight değerleri oluşur, düşük korelasyonlu (yani ilişki belirtmeyen rastgele dağılımlı) çiftler arasında da düşük weight parametreleri oluşur. Burda sinir ağımız ortadaki lamba ile sonuç arasında önemli bir korelasyon olduğunu belirtiyor; her adımda bu korelasyonu fark ederek gittikçe bunun değerini bire, diğerlerininkini de sıfıra yaklaştırıyor. Böylece tahmin adımında input değerlerini karşılık gelen weight değerleri ile çarpıp topladığımızda output değerine yaklaşık sonuç elde edebiliriz.\n",
    "\n",
    "Peki korelasyona bağlı bu değişimi sinir ağımız nasıl yapabildi? Cevap *gradient descent* adımı; bu işlem her bir weight değeri üzerinde aşağı ya da yukarı yönde bir baskı oluşturur. Bu baskı sonucunda ilgili weight değeri küçülür veya büyür. Gradient descent adımında toplam hata tespit edilir ve bu hataya hangi weight değerlerinin hangi oranlarda katkıda bulunduğu tespit edilir. Daha sonra hataya katkıda bulunan weight değerleri küçültülür, katkısı olmayanlar bırakılır.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{cccc|cccc}\n",
    "%\\textrm{Data}&&&&&\\textrm{Weight baskısı}&&\\\\\n",
    "\\texttt{1, 0, 1} & \\longrightarrow & \\texttt{0} & \\qquad & \\qquad & \\texttt{--}\\quad\\texttt{0}\\quad\\texttt{--} & \\longrightarrow & \\texttt{0}\\\\\n",
    "\\texttt{0, 1, 1} & \\longrightarrow & \\texttt{1} & \\qquad & \\qquad & \\texttt{0}\\quad\\texttt{+}\\quad\\texttt{+} & \\longrightarrow & \\texttt{1}\\\\\n",
    "\\texttt{0, 0, 1} & \\longrightarrow & \\texttt{0} & \\qquad & \\qquad & \\texttt{0}\\quad\\texttt{0}\\quad\\texttt{--} & \\longrightarrow & \\texttt{0}\\\\\n",
    "\\texttt{1, 1, 1} & \\longrightarrow & \\texttt{1} & \\qquad & \\qquad & \\texttt{+}\\quad\\texttt{+}\\quad\\texttt{+} & \\longrightarrow & \\texttt{1}\\\\\n",
    "\\texttt{0, 1, 1} & \\longrightarrow & \\texttt{1} & \\qquad & \\qquad & \\texttt{0}\\quad\\texttt{+}\\quad\\texttt{+} & \\longrightarrow & \\texttt{1}\\\\\n",
    "\\texttt{1, 0, 1} & \\longrightarrow & \\texttt{0} & \\qquad & \\qquad & \\texttt{--}\\quad\\texttt{0}\\quad\\texttt{--} & \\longrightarrow & \\texttt{0}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Yukarıdaki tabloda sol tarfta input ve output değerleri, sağ tarafta da ilgili weight baskısı canlandırılıyor. Mesela ilk training örneğine bakın; input (1,0,1) ve output (0) arasında korelasyonu tespit etmek için ilk ve üçüncü weight değerleri azaltılıyor, ortadaki weight değeri ise sabit bırakılıyor çünkü bu zaten sıfır (weight ne olursa olsun bununla çarpılınca etkisi olmayacak). Altı training örneğinin tamamını dikkate aldığımızda şunu fark ederiz; sağ ve sol tarafta hem pozitif hem de negatif baskılar var ve bunların tamamı bunları sıfıra doğru sürükler, ortadaki weight parametresi ise her adımda yükseltilmiş. Yani makine ortadaki lamba ile sonuç arasında sürekli korelasyon uyarısı vermiş. Sinir ağımız her adımda korelasyon bulduğu yerde weight değerini yükselterek 1'e yaklaştırmış, korelasyon görmediği (diskorelasyon) yerlerde weight değerlerini azaltmış. İşlemin sonunda da toplam olarak sinir ağı ortadaki input ile sonuç arasında güçlü bir korelasyon bulduğunu belirtmiş. Tahmin dediğimiz şeyin input ile weightlerin çarpımı olduğunu hatırlayın, bu yapılan işin mantıklı olduğunu kavrayacaksınız.\n",
    "\n",
    "peki ya bir adımda tesadüfen hata sıfır çıkarsa? Mesela yukarıdaki tablonun ilk training örneğinde sağdaki weight değeri 0.5 ve soldaki weight değeri de -0.5 olsaydı ne olurdu? Bu durumda weight ile input çarpımı 0 olacak, yani weight değerlerini güncellemeye gerek kalmadan mükemmel tahmini (error=0) yakalamış oluruz. Weight değerleri güncellenmediği için burada öğrenme gerçekleşmemiş olur, eğer bu yakalanan hatasız tahmin tesadüfi ise makine hiç bir şey öğrenmedi ve başka inputlarda hatalı tahmin yapacak demektir. Bu olguya ***overfitting*** denir. Eğer sadece bu veriyi kullanarak öğrenme yaptıysak yanlış yaptık, peki diğer verileri kullanarak devam edersek ne olur? Bu durumda diğer input değerleri overfitting olmuyorsa weight değerlerini güncellemeye devam eder ve makine öğrenmeye devam eder. Öğrenmeyi mesela sadece ilk iki inputla yaparsanız makineyi bu iki durumu ***ezberlemeye*** (memorize) zorlamış olursunuz, bu durumda bu ikisi dışında daha genel inputlar için hatalı tahmin üreteceksiniz demektir. Amacımız inputları kullanarak çok daha genel tahmin yapabilecek genel bir kural saptamak olmalıdır (generalization).\n",
    "\n",
    "Şimdi yukarıdaki training tablosunda bir şeye daha dikkat çekmek istiyorum. Ortadaki weight değerleri üzerinde hep pozitif baskı var ve doğal olarak bunlar artarak bire yaklaşıyor, soldaki weight değerleri üzerinde hem pozitif hem de negatif baskılar var fakat negatifler daha fazla olduğundan bu weight değeri azalarak sıfıra yaklaşıyor. Fakat sağdaki weight değeri üzerinde negatif ve pozitif baskılar eşit oranda, sinir ağında bu weight değeri sıfıra yakınsıyor. Bunun anlamı bu weight üzerindeki negatif baskıların şiddeti pozitiflere göre daha yüksek, peki bu nasıl oluşuyor? Gradient descent adımında sağ ve orta weight değerleri ideal değerlerine yaklaştıkça bunların toplam hataya katkısı azalacak ve sol weight değerinin hataya katkısı daha görünür olacaktır, dolayısıyla bundan sonra soldakinin negatif veight baskısı artacaktır. Aşağıdaki grafikte bunu gözlemleyebilirsiniz, soldaki weight değeri diğerlerine göre daha yavaş yakınsıyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weights = np.array([0.5,0.48,-0.7])\n",
    "alpha = 0.1\n",
    "\n",
    "streetlights = np.array( [[ 1, 0, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 0, 0, 1 ],\n",
    "                          [ 1, 1, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 1, 0, 1 ] ] )\n",
    "\n",
    "walk_stop = np.array( [0, 1, 0, 1, 1, 0] )\n",
    "\n",
    "input = streetlights[0] # [1,0,1]\n",
    "goal_prediction = walk_stop[0] # 0\n",
    "\n",
    "wr = []\n",
    "wm = []\n",
    "wl = []\n",
    "\n",
    "for iteration in range(40):\n",
    "    error_total = 0\n",
    "    for i in range(len(walk_stop)):\n",
    "        input = streetlights[i]\n",
    "        truth = walk_stop[i]\n",
    "        prediction = input.dot(weights)\n",
    "        error = (truth - prediction)**2\n",
    "        error_total += error\n",
    "        delta = prediction - truth\n",
    "        weights = weights - (alpha*(input*delta)) #stochastic gradient descent\n",
    "        wr.append(weights[0])\n",
    "        wm.append(weights[1])\n",
    "        wl.append(weights[2])\n",
    "        \n",
    "plt.plot(range(len(wr)), wr, range(len(wm)), wm, range(len(wl)), wl)\n",
    "plt.legend(['left', 'mid', 'right'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukarıdaki ilk training örneğinde ortadaki input böyle mükemmel bir korelasyona sahip olmasaydı sinir ağımız sağdaki weight değerini sıfıra yaklaştırmakta başarısız olabilirdi. Daha sonra ***regularization*** işlemini açıklayacağım, bu işlem aynı sayıda pozitif ve negatif baskıya sahip weight değerlerini doğrudan sıfıra yaklaşmaya zorlar. Bu işlem avantajlıdır çünkü bu sayede sadece güçlü korelasyona sahip weight değerlerini günceller, korelasyona sahip olmayanları sustururuz. Bu örnekte ortadaki input korelasyona sahip olmasaydı regularization yapmadan sağdaki weight değerini sıfırlamamız zor olabilirdi, regularization ile bu değer doğrudan sıfıra yaklaştırılır.\n",
    "\n",
    "Şimdi aşağıdaki dataseti inceleyin, bu durumda sinir ağımız ne yapabilir?\n",
    "\n",
    "$$\n",
    "\\begin{array}{cccc|cccc}\n",
    "%\\textrm{Data}&&&&&\\textrm{Weight baskısı}&&\\\\\n",
    "\\texttt{1, 0, 1} & \\longrightarrow & \\texttt{1} & \\qquad & \\qquad & \\texttt{+}\\quad\\texttt{0}\\quad\\texttt{+} & \\longrightarrow & \\texttt{1}\\\\\n",
    "\\texttt{0, 1, 1} & \\longrightarrow & \\texttt{1} & \\qquad & \\qquad & \\texttt{0}\\quad\\texttt{+}\\quad\\texttt{+} & \\longrightarrow & \\texttt{1}\\\\\n",
    "\\texttt{0, 0, 1} & \\longrightarrow & \\texttt{0} & \\qquad & \\qquad & \\texttt{0}\\quad\\texttt{0}\\quad\\texttt{--} & \\longrightarrow & \\texttt{0}\\\\\n",
    "\\texttt{1, 1, 1} & \\longrightarrow & \\texttt{0} & \\qquad & \\qquad & \\texttt{--}\\quad\\texttt{--}\\quad\\texttt{--} & \\longrightarrow & \\texttt{0}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Gördüğünüz gibi hiç bir input ile output arasında en ufak korelasyon yok, ayrıca her weight değeri eşit sayıda pozitif ve negatif baskıya sahip. Bu dataset sinir ağımız için büyük bir baş belası! Böyle bir durumda ne yapılabilir?\n",
    "\n",
    "Eğer input dataset ile output dataset arasında hiç korelasyon yoksa yada çok düşük korelasyon varsa biz input dataseti kullanarak ara bir dataset üretiriz (farklı boyutta olabilir), bu yeni yapay dataset ile output arasında bir miktar korelasyonun var olmasını sağlarız. Böylece output ile korelasyonu bulunmayan bir dataset yerine bundan üretilmiş fakat korelasyona sahip bir dataset kullanarak training gerçekleştiririz. Sonuçta sinir ağımız üç aşamalı olur, sinir ağlarında bunlara ***katman*** (layer) deriz (burada input dataset, ara dataset ve output dataset olmak üzere üç katman var). Buradaki `layer_1` gibi ara katmanlara gizli katman (***hidden layer***) deriz.\n",
    "\n",
    "![](img/deep_learning_13.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukarıda sinir ağımızın yapısı gösteriliyor; bu şekilde ikiden fazla katmana sahip sinir ağlarına ***derin sinir ağları*** (deep neural networks) denir, bu ağlarla gerçekleşen makine öğrenmesine de ***derin öğrenme*** (deep learning) denir. Bu örnekte `layer_0` ile `layer_2` arasında korelasyon yok fakat `layer_0` katmanını kullanarak ürettiğimiz `layer_1` katmanı ile `layer_2` arasında belirli bir korelasyon var, peki pratikte bu özelliğe sahip `layer_1` katmanını nasıl oluşturacağız? Bu derin sinir ağında öğrenme nasıl gerçekleşir?\n",
    "\n",
    "Bu sorulara cevap aramak için sinir ağına daha yakından bakalım. Burada `layer_1` ile `layer_2` katmanlarından oluşan kısım daha önce defalarca train ettiğimiz bir yapıdadır; `layer_1` inputlarını alırız ve `weights_1_2` ile işleyerek tahmin üretiriz, daha sonra hatayı tespit ederek ilgili `delta` parametresine göre bir döngü içinde `weights_1_2` parametrelerini güncelleyerek training yaparız.\n",
    "\n",
    "Burada yeni olan kısım `layer_0` ile `layer_1` katmanlarından oluşan parçadır, `weights_0_1` parametrelerini nasıl güncellemeliyiz ki `layer_0` inputlarını bu weight ile işleyince oluşan `layer_1` katmanı ve `layer_2` katmanı arasında biraz korelasyon olsun? Bu `weights_0_1` parametrelerini güncelleyebilmek için karşılık gelen `delta` (layer_1 için) parametrelerini bilmemiz gerekir, böylece gradient descent ile weight parametrelerini güncelleriz. Şimdi `layer_1` için `delta` parametrelerini nasıl tespit edebileceğimize bakalım, tüm weightlerin rastgele başlatıldığını varsayalım. Biliyoruz ki `layer_2` tahminindeki hataya `layer_1` katmanındaki yüksek weight değerine sahip inputlar daha çok katkı sağlar, dolayısıyla `layer_2` tahminini (dolayısıyla hatayı) azaltmak bir miktar istersek `layer_1` inputlarını ilgili weight değerleri oranında değiştirmemiz gerekir. Çünkü bu tahmin `layer_1` inputlarının `weight_1_2` ile işlenmesiyle elde ediliyor. Dolayısıyla `layer_1` katmanının `delta` değerlerini `layer_2` katmanının `delta` değerlerinin karşılık gelen `weights_1_2` değerleriyle çarparak tespit ederiz. Bunları kullanarak da `weights_0_1` parametrelerini güncelleyebiliriz. Yani sonuca bakarak daha gerideki katmanın nasıl değiştirileceğini belirliyoruz, tersten düşünerek geliştirdiğimiz bu yönteme ***back propagation*** denir.\n",
    "\n",
    "Şimdi daha geniş bir pencereden sinir ağımıza bir daha bakalım. Bir input değeri (`layer_0`) ilhili bir weight değeriyle (`weights_0_1`) çarpılıp bir değer elde ediliyor (`layer_1`), daha sonra bu değer de ilgili bir weight ile (`weights_1_2`) çarpılarak output değeri (`layer_2`) sonucuna ulaşılıyor. Daha sonra bu işlem diğer input we weightler için tekrarlanıyor. Sonuç olarak input değerinden output değerine ulaşmak için iki defa farklı weight değerleri ile çarpma işlemi yapılıyor, tek katmanlı bir ağdan tek farkımız bu! Oysa iki çarpmanın yapacağı işi tek bir çarpma ile de yapabiliriz, mesela $$1.0\\cdot 0.25\\cdot 0.9 = 1.0\\cdot 0.225 = 0.225,$$ dolayısıyla tek katmanlı bir sinir ağı oluşturup tek bir uygun weight çarpmasıyla başarabileceğimiz işi iki katmanlı bir sinir ağıyla yapmaya çalışıyoruz. Yani iki katman kullanmamızın pek bir fayda sağladığı yok aslında!\n",
    "\n",
    "Bir de şu açıdan düşünelim; `layer_0` ile `layer_2` arasında korelasyon yok, biz `layer_0` katmanını `weights_0_1` matrisi ile çarparak `layer_1` katmanı oluşturuyoruz. Dolayısıyla `layer_1` katmanı aslında `layer_0` katmanıyla bir miktar korelasyona sahip olan bir katmandır. Matris çarpımı lineer bir işlemdir, katmanları dönüştürürken bunların temel yapısını korur. Sonuçta korelasyonsuz bir katmandan elde ettiğimiz katman aslında kullanışsız olan bir korelasyona sahip oluyor, bu korelasyonun `layer_2` için hiç bir faydası yok. Bu haliyle sinir ağımızı çalıştırırsak yakınsayamaz!\n",
    "\n",
    "Bundan dolayı burada bir hamle daha yapmamız gerekiyor, bu hamle `layer_1` katmanının `layer_0` ile tamamen korelasyonu olmasını engellemeli. Bunun için en yaygın yöntemlerden birisi şudur; input `layer_0` değerlerinin tamamıyla değil de sadece bazılarıyla korelasyon sağlamak, bunun için çarpma sonrasında bazı `layer_1` değerleri sıfır yapılır. Böylece dönüşüm doğrusallıktan kurtulmuş olur ve `layer_1` katmanı `layer_0` katmanının bir kopyası gibi davranmaz. Bu işlem iş akışına biraz ***nonlineerlik*** (nonlinearity) katar, doğrusallığı engellemenin başka yolları da vardır ve daha sonra bazılarına değineceğiz.\n",
    "\n",
    "Şöyle yapacağız, `layer_1` değerlerinden negatife düşenleri sıfırlayacak ve diğerlerine müdahale etmeyeceğiz. Böylece yeni katman (`layer_1`) değerleri `layer_0` katmanı değerlerinin sadece bazıları ile korelasyona sahip olacak. Bu gibi lineerliği bozan işlemlere makine öğrenmesi dilinde aktivasyon (***activation***) denir. Burada yaptığımız aktivasyon da en meşhurlarından biridir ve ***relu*** (rectified linear unit) aktivasyon işlemi denir. İki katmanlı bir sinir ağı kullanmanın gerçek avantajı bu işlemdir, bunu tek katmanlı bir ağla yapamayız.\n",
    "\n",
    "Aşağıdaki program anlattığım şekilde tahminde bulunuyor (forward propagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39194327]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0)*x\n",
    "\n",
    "streetlights = np.array( [[ 1, 0, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 0, 0, 1 ],\n",
    "                          [ 1, 1, 1 ] ] )\n",
    "\n",
    "walk_vs_stop = np.array([[ 1, 1, 0, 0]]).T\n",
    "\n",
    "weights_0_1 = 2*np.random.random((3, 4)) - 1 #random weights\n",
    "weights_1_2 = 2*np.random.random((4, 1)) - 1 #random weights\n",
    "\n",
    "layer_0 = streetlights[0] #input\n",
    "layer_1 = relu(np.dot(layer_0, weights_0_1)) #relu activation\n",
    "layer_2 = np.dot(layer_1, weights_1_2) #output\n",
    "\n",
    "print(layer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/deep_learning_14.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aşağıdaki program hem forward propagation hem de back propagation yapıyor. İki katmanlı bir derin sinir ağının yapması gereken her şeyi yapıyor yani."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.6342311598444467\n",
      "Error:0.35838407676317513\n",
      "Error:0.0830183113303298\n",
      "Error:0.006467054957103705\n",
      "Error:0.0003292669000750734\n",
      "Error:1.5055622665134859e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0)*x # returns x if x > 0\n",
    "                       # return 0 otherwise\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output>0 # returns 1 for input > 0\n",
    "                    # return 0 otherwise\n",
    "\n",
    "streetlights = np.array( [[ 1, 0, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 0, 0, 1 ],\n",
    "                          [ 1, 1, 1 ] ] )\n",
    "\n",
    "walk_vs_stop = np.array([[ 1, 1, 0, 0]]).T\n",
    "\n",
    "alpha = 0.2\n",
    "hidden_size = 4\n",
    "\n",
    "weights_0_1 = 2*np.random.random((3, hidden_size)) - 1\n",
    "weights_1_2 = 2*np.random.random((hidden_size, 1)) - 1\n",
    "\n",
    "for iteration in range(60):\n",
    "    layer_2_error = 0\n",
    "    for i in range(len(streetlights)):\n",
    "        layer_0 = streetlights[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "        layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1])**2)\n",
    "        layer_2_delta = (walk_vs_stop[i:i+1] - layer_2)\n",
    "        layer_1_delta=layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1) #back propagation\n",
    "        weights_1_2 += alpha*layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha*layer_0.T.dot(layer_1_delta)\n",
    "    if(iteration%10 == 9):\n",
    "        print(\"Error:\" + str(layer_2_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada back propagation yapılan adıma dikkat edin; `layer_1` değerlerinde relu aktivasyonu yaparak ve negatif değerlerini sıfırlamıştık. Dolayısıyla bunların son hataya hiç katkısı yok, bu nedenle bunların delta değerlerini de sıfır yapmalıyız. Bunu yapmak için `relu2deriv` fonksiyonunu tanımladık. Bu satırın dışında bu programın öncekilerden farklı bir yanı yok!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi derin sinir ağlarının önemini bir örnek üzerinde kabaca açıklayacağım. Aşağıdaki resimde bir kedi olup olmadığını tahmin edecek bir sinir ağı kurmak istediğinizi varsayın, bur bir kedi resmi olduğu için tahminin `1` veya çok yakın olması gerekir. Kedi var olmasını `1` aksi durumu `0` ile kodladığımızı düşünüyoruz.\n",
    "\n",
    "![](img/deep_learning_15.jpg)\n",
    "\n",
    "Sinir ağımızı train etmek için de bir datasetimiz var olsun ve bunun içinde kedi resimleri ve kedi içermeyen resimler olsun, bol miktarda. Bunun içindeki kedi resimleri aşağıdaki gibi olabilir.\n",
    "\n",
    "![](img/deep_learning_16.jpg)\n",
    "\n",
    "Önce input datasetimizi hazırlamalıyız; bunun için her bir kedi resmini belirli bir boyutta sabitleyip her bir pikselinin değerini okumalıyız. Daha sonra her bir piksel değerini içeren bir list oluşturup bunu o resmin datası olarak kaydederiz, daha sonra da tüm resimlerin datalarını bir list içinde toplayarak input dataset oluştururuz. Output dataset için de her bir resme karşılık gelen 0 veye 1'lerden oluşan bir list hazırlarız.\n",
    "\n",
    "![](img/deep_learning_17.jpg)\n",
    "\n",
    "Son incelediğimiz örnekte olduğu gibi burada da hiç bir piksel ile output arasında bir korelasyon yoktur, sadece bu piksellerden elde edilen yeni ara dataset içindeki piksellerde bir korelasyon oluşabilir. Derin öğrenmenin mantığı budur, ara katmanlarla korelasyon oluşturmak. Bu örnekte çok katmanlı derin bir ağ kullansaydık ara katmanlar tüm bir kediyi değil bir kedi gözünü, kuyruğunu veya kulağı gibi parçaları temsil ediyor olabilir ve kedinin tamamı ile korelasyon aramak yerine bu gibi parçalar için korelasyon aramak sinir ağının yetneğini geliştirir. Biz insanlar da nesneleri böyle tanırız aslında, ilk defa kedi gören bir çocuk ikinci kez fakat farklı bir kedi gördüğünde bunun bir kedi olduğunu önce gördüğüne benzer uzuvlara sahip olduğu için anlayabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bundan sonra sinir ağlarını daha önce gösterdiğim gibi detaylı grafiklerle değil aşağıdaki gibi bir grafikle göstereceğim.\n",
    "\n",
    "![](img/deep_learning_18.jpg)\n",
    "\n",
    "Çok fazla açıklama yapmaya gerek yok sanırım, her şey görselde açıklanmış. Hatta bunu daha da basitleştirebilirim, açık olarak boyutları belirtmeye gerek yoktur. Çünkü mesela 3 elemanlı `layer_0` ile 4 elemanlı `layer_1` katmanlarını bağlayan `weights_0_1` matrisi boyutunun (3, 4) olması gerektiği açıktır. Bundan dolayı aşağıdaki gibi daha basit bir görsel kullanılabilir.\n",
    "\n",
    "![](img/deep_learning_19.jpg)\n",
    "\n",
    "Bunun da ötesinde, bazen sinir ağlarını matematiksel olarak ifade edeceğim. Mesela bu derst ele aldığımız örnekte `weights_0_1` matrisini $W_0$ ve `weights_1_2` vektörünü de $W_1$ ile gösterelim; ayrıca `layer_0`, `layer_1` ve `layer_2` vektörlerini de sırasıyla $l_0$, $l_1$ ve $l_2$ ile gösterelim. Bu notasyona göre sinir ağımız \n",
    "\n",
    "$$l_1=\\textrm{relu}\\left(l_0W_0\\right)$$\n",
    "\n",
    "ve\n",
    "\n",
    "$$l_2=l_1W_1$$\n",
    "\n",
    "işlemlerini yapıyor. Bu ikisini birleştirirken sinir ağının tüm forward propagation işlemi\n",
    "\n",
    "$$l_2=\\textrm{relu}\\left(l_0W_0\\right)W_1$$\n",
    "\n",
    "olarak ifade edilebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir sinir ağı birden fazla ara katmana (hidden layer) sahip olabilir, bazı sinir ağları çok sayıda gizli katmana sahiptir. Sinir ağının tasarımına bunun ***yapısı*** (structure) denir, araştırmacılar genel olarak ele alınan probleme göre en iyi korelasyon oluşturacak şekilde sinir ağı tasarlamayla uğraşır. Bir sinir ağı tasarımında bir çok parametre vardır; gizli katman sayısı, gizli katmanların boyutu, iterasyon sayısı, aktivasyon çeşidi gibi. Ele alınan probleme göre bunların optimum seçimi yapılması gerekir. Bundan sonraki ders notlarımda hangi problemlerde hangi tasarımların avantajlı olduğuna değineceğim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
